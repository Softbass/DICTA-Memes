% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{Data-Intensive Computing for Text Analysis}{Fall 2011, School of Information, University of Texas at Austin}
\CopyrightYear{2011} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Meme Tracking in Scale with MapReduce}% \titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
\subtitle{Progress Report}
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Hohyon Ryu\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{hohyon@utexas.edu}
\alignauthor
Jae Hyeon Bae\\
       \affaddr{Computer Science}\\
       \affaddr{University of Texas at Austin}\\
       \email{metacret@gmail.com}
\alignauthor
Nicholas Woodward\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{woodward.nicholas@gmail.com}
}

% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{14 September 2011}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}

This article is to report our project progress for Fall 2011 Data-Intensive Computing for Text Analysis class. We have completed preprocessing of the Blogs08 collection by removing all HTML tags, boilerplate text, non-English documents, and duplicates. After the preprocessing, aproximately 4 million popular common phrases were extracted using an enhanced version of the technique suggested by \cite{Kolak2008}. The extracted phrases are clustered in 2 million meme groups. The meme groups will be ranked automatically using Support Vector Machine, based on the crowdsourced labels at Amazon Mechanical Turk.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4.m}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{H.5.4}{Information Interfaces and Presentation}{Hypertext/Hypermedia}

\terms{MapReduce, Blogosphere}

\keywords{Keyword Extraction, Data-intensive Processing, Implicit links in Blogosphere, Meme tracking}

\section{Introduction}

Web logs, also known as blogs, are a very important and interesting source of diverse information. They provide insight into the culture of the society in which they exist. People search blogs to find out how to cook, what to do, what to see, and to discover what others think. Because blogs are typically written in plain language by individuals enthusiastic for their interest, they may be more easily understood than longer, more-detailed news articles and cover a wider personal range of topics. 
As blogs are highly sensitive to fast-changing temporal issues, finding and tracking memes, pieces of information that travel across blog posts, we can mine variety of issues and fads in the blogosphere.

** Nick and Jae, please help fill out the intro part.


\section{Meme Extraction}

\subsection{Test Collection}

BLOGS08\footnote{http://ir.dcs.gla.ac.uk/test\_collections/blogs08info.html} is a TREC\footnote{http://trec.nist.gov/} test collection that crawled Web Logs from Jan/14/2008 to Feb/10/2009. It consists of 3 components: feeds, permalink documents, and homepage documents. We use only permalink documents that contain 28,488,766 documents and whose uncompressed size is 1445GB. Because the collection contains many duplicates, non-English blog posts, advertisement, and Blog website frameworks, we will use NLTK and Decruft to extract English blog contents without boilerplate text.

\subsection{Preprocessing Blogs08}

Preprocessing of the Blogs08 Collection was completed using Python with NLTK and Decruft packages. Decruft extracts only meaningful content from a blog page, filtering out navigation links, advertisements, sidebar contents, and links to other sites. Using a language identification module, NLTK checks if the extracted content is written in English. Blogs08 also contains a tremendous amount of duplicate documents and documents without a significant amount of text. We set a lower-bound of 5 words and removed duplicates, also using Python. After all these preprocessing steps were completed, extracted contents of a blog post are written into a single line per document for the MapReduce meme extraction process. The number of Decruft-applied, deduplicated, and HTML stripped English blog posts is 15,097,266 (43 GB).

\subsection{Meme Extraction with MapReduce}

Preprocessed Blogs08 data is comprised of key-value pair where the key is document number(docno) and the value is the preprocessed content of the blog posts. Using Hadoop Streaming, Python MapReduce code extracts common phrases and group documents by them. Overall algorithm design follows that of \cite{Kolak2008}. However, because of a scalability issue, lower bounds and upper bounds limit the output size of the data, and other optimizations are used such as stop word ratio check and preposition trimming. The 3 steps of MapReduce pseudo-code for Meme Extraction is illustrated in the following pseudo codes. The first MapReduce splits the input text into sentences and extract trigrams and their index within the document. The reducer cuts off the common phrases($trigrams$) that appear less than 5 times or more than 300,000 times.

\begin{centering}

\begin{tabbing}
\textbf{MapReduce 1:}\\
\emph{Mapper 1:}\\

for \= ($doc\_id$, $text$) in [input file]:\\
\>	$sentences$  $\leftarrow$ split text by sentence boundary\\
\\
\>	for \= $sentence$ in $sentences$:\\
\>\>	$index$ $\leftarrow$ 0\\
\>\>	for \= $trigrams$ in $sentence$:\\
\>\>\>		$index$++	\\
\>\>\>		emit ($trigrams$, ($doc\_id$, $index$))\\
\\
\emph{Reducer 1:}\\
$lower\_bound$  $\leftarrow$ 5\\
$upper\_bound$  $\leftarrow$ 300000\\
for \= ($trigrams$, [($doc\_id$, $index$)]) in [mapper 1 output]:\\
\>	$len \leftarrow$ count([($doc\_id$, $index$)])\\
\> 	if \= $lower\_bound$ < $len$ < $upper\_bound$:\\
\>\>	emit ($trigrams$, [($doc\_id$, $index$)])\\
\end{tabbing}
\end{centering}

The second MapReduce sorts the trigrams and indices by document to concatenate adjacent trigrams in MapReduce 3:

\begin{centering}
\begin{tabbing}
\textbf{MapReduce 2:}\\
\emph{Mapper 2:}\\
for \= ($trigrams$, [($doc\_id$, $index$)]) in [reducer 1 output]:\\
\>	for \= ($doc\_id$, $index$) in [($doc\_id$, $index$)]:\\
\>\> emit ($doc\_id$, ($trigrams$, $index$))\\
\\
\emph{Reducer 2:}\\
for \= ($doc\_id$, [($trigrams$, $index$)]) in [mapper 2 output]:\\
\>	emit ($doc\_id$, [($trigrams$, $index$)])

\end{tabbing}

\end{centering}

The third MapReduce concatenates the adjacent trigrams within a blog post. Mapper 3 concatenates adjacent trigrams into a meme. Before emitting the memes, a function trimPrepositions trims off prepositions at the beginning and end of the meme, and a function stopwordRate checks how many of the words in the meme are stop words. If stop words take more than 65\% of the words in the meme, the meme is ignored. In the reducer 3, phrases that are too short ($\leqq 5$) or too long ($\geqq 200$) are ignored as they are often timestamps, advertisement, or boiler plate text. The final MapReduce is illustrated as follows:

\begin{centering}
\begin{tabbing}
\textbf{MapReduce 3:}\\
\emph{Mapper 2:}\\

for \= ($doc\_id$, [($trigrams$, $index$)]) in [reducer 2 output]:\\
\>	$prev\_index=0$\\
\>	$meme=``"$\\
\>	for \= ($trigrams$, $index$) in [($trigrams$, $index$)]:\\
\>\>	if \= $index-prev\_index \leqq 3$:\\
\>\>\>		$meme$ += $trigrams$[3-($index-prev\_index$):]\\
\>\>	else:\\
\>\>\>		$meme$=trimPrepositions($meme$)\\
\>\>\>		if \= stopwordRate($meme$)<0.65:\\
\>\>\>\>		emit($meme$, $doc\_id$)\\
\>\>\>		$meme$=trimPrepositions($meme$)\\
\>\>\>		$meme$.append($meme$)\\
\>\>\>		$meme$=$trigram$\\
\>\>	$prev_index=index$\\
\\	
\>	\# clean up for the last one\\
\>		$meme$ = trimPrepositions ($memes$)\\
\>		if \= stopwordRate($meme$)<0.65:\\
\>\>		emit($meme$, $doc\_id$)\\
\\
\emph{Reducer 3:}\\
for \= ($meme$, [($doc\_id$)]) in [mapper 3 output]:\\
\> if \= $5 \leqq len(meme) \leqq 200$:\\
\>\>	emit ($meme$, [($doc\_id$)])

\end{tabbing}

\end{centering}

The final output is key, value pairs of a meme and the list of the document ids that the meme appears. In total, 4,610,100 memes are extracted from the TREC Blogs08 collection.

\section{Meme Clustering}

Since the form of memes evolve over time across different media \cite{Leskovec2009}, similar memes need to be clustered together. For example, ``lipstick look good on a pig," ``lipstick on the pig," and "lipstick on this pig" need to be seen in the same cluster. Canopy clustering algorithm \cite{McCallum2000} is used for this meme clustering. However, since we are clustering more than 4.6 million memes into many clusters of very fine granularity, different strategies should be adopted. Since Canopy is basically a single pass approach, the distance between every meme and every cluster needs to be calculated in $O(n^2)$ computational complexity. Meme clustering task is very different from ordinary document clustering task as the number of documents is extremely high while the length of each document is usually fewer than a dozen of words. We devised following optimizations and modifications to efficiently cluster 4.6 million memes:
\begin{itemize}
 \item Binary vector: A vector is simply a set of words in a meme or a centroid. The set operation is O(1) while list operation is O(n). Also word order is ignored by this method.
 \item Centroid indexing: The centroid is indexed by the words it includes so that when a meme is compared to only with the cluster centroids that include common words.
 \item New similarity coefficient: Jaccard similarity coefficient 
\begin{displaymath} 
 J(A,B) = {{|A \cap B|}\over{|A \cup B|}}
\end{displaymath} 
  is inappropriate for meme clustering. We compare a meme with a cluster centroid, and when a cluster centroid is long, short memes cannot have high coefficient scores. Therefore, instead of dividing by the union, we divide the number of common words by the average length of the two vectors:\\
\begin{displaymath}
memeSim (A, B) = {{|A \cap B|} \over {(|A|+|B|) / 2} }
\end{displaymath}
The similarity score should be higher than 0.6 to be clustered into a given centroid.
 
 \item Stop word removal: The memes should be compared by meaningful words instead of stop words. Otherwise, ``You can put lipstick on a pig'' and ``You can put a book on the table" would be clustered together. Using an extended stop word list that includes month names, week day names, we excluded stop words from the similarity computation. 
 \item Distributed computation: To cluster 4.6 millions of memes efficiently, mapreduce-based distributed Canopy approach, explained in Apache Mahout Canopy Clustering\footnote{https://cwiki.apache.org/MAHOUT/canopy-clustering.html}, is used. The mapper takes in a partial list of the memes. Using canopy algorithm, it generates cluster centroids. The reducer does the same clustering process with the aggregated centroids. The pseudo code of the MapReduce process is illustrated as follows:
 

\begin{centering}
\begin{tabbing}
\textbf{Meme-clustering:}\\
\emph{Mapper:}\\

$CIndex \leftarrow new HashMap$ \# Centroid Index\\
$C \leftarrow new List$ \# Cluster Centroids\\
for \= $meme$ in $memes$:\\
\>	\# Initialize $C$ and $CIndex$\\
\>	\# 0 is the index of the first cluster\\
\>	if \= $C$ is empty:\\
\>\>	\# set() makes a set of words in a meme\\
\>\>	$C$.append( set(meme) )\\
\>\>	for \= $word$ in $meme$:\\
\>\>\>		$CIndex$[$word$].add(0) \\
\\
\>	\# Get candidate centroids from $CIndex$\\
\>	$Candidates \leftarrow new Set()$\\
\>	for \= $word$ in $meme$:\\
\>\>		for \= $c$ in $CIndex$[$word$]:\\
\>\>\>		$Candidates$.add($c$)\\
\\
\>	$flag \leftarrow False$\\
\>	for \= $c$ in $Candidates$:\\
\>\>	if memeSim($meme$, $C$[$c$])>0.6:\\
\>\>\>		$C$[$c$]=$C$[$c$] + set($meme$)\\
\>\>\>	$flag \leftarrow True$\\
\>\>\>		for \= $word$ in $meme$:\\
\>\>\>\>		$CIndex$[$word$].add($c$) \\
\>\>\>		break
\\
\> if \= $flag == False$:\\
\>\>	$C$.append(set($meme$))\\
\>\>	$c$=len($C$)-1\\
\>\>		for \= $word$ in $meme$:\\
\>\>\>		$CIndex$[$word$].add($c$) \\
emit($C$)\\

\\
\emph{Reducer:}\\
\>Reducer is indentical to the mapper except that \\
\> the input is the centroids($C$) instead of\\
\> the memes($memes$).\\

\end{tabbing}

\end{centering}

 
\end{itemize}




\section{Meme Ranking}
Even though we have successfully clustered memes, we still have many things to do for finding valuable and meaningful information from meme clusters. Basically, we can classify every meme cluster as informative or not because there are many meaningless sequence of words such as ''2 1956 jamboree tues jan 3 1956 jamboree wed,oct 5th wed oct 4 tues``. If we extract some statistical features from meme clusters and build the training data indicating informativeness, we can predict informativeness of each meme cluster. Finally, if we build the model of classifier as regression problem of probability of informativenss given meme cluster, we can bring order into a set of meme clusters with the probability values. Currently, we are expecting the following features as discriminant to decide meme cluster's informativeness.
\begin{itemize}
 \item the number of memes in the cluster
 \item average meme length
 \item average number of documents in the cluster
 \item average number of sources in the cluster
 \item meme cluster cohesiveness
 \item time span information of blog documents containing memes in the cluster
  \begin{itemize}
    \item the number of days blog documents were written
    \item the number of documents per day
  \end{itemize}
\end{itemize}
Meme cluster cohesiveness can be defined as 
\begin{displaymath}
\frac{the\ number\ of\ unique\ words}{the\ number\ of\ words}
\end{displaymath}
which means how similar memes in the cluster are each other.

\subsection{Building Training Set at Amazon Mechanical Turk}

Nick's part

\subsection{Automatic Meme Ranking using Support Vector Machine}
We will use Support Vector Machine for meme ranking. Armed with LIBSVM package\footnote{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}, we will train C-SVM and regard each probability value as the ranking score. For better performance, we should scale feature values and use grid search to find suitable parameters.


\section{Evaluation}

Based on the labels collected at AMT, the performance on SVM-based ranking will be evaluated.

\section{Project Plan}

\subsection{Division of Work}

\begin{itemize}
\item Hohyon Ryu: Data Preprocessing, Meme Extraction, Meme Clustering
\item Jae Hyeon Bae: Theoretical Background, SVM-based Meme Ranking
\item Nicholas Woodward: Theoretical Background, Crowdsourcing the meme cluster evaluation
\end{itemize}

\subsection{Timeline}


\begin{itemize}
  \item Oct/6: Preprocessing - Done
  \item Oct/13: Meme Extraction
  \item Oct/20 \& 27: Meme Clustering
  \item Nov/3 \& 10: Crowdsourcing 
  \item Nov/10: SVM-based Meme Cluster Ranking
  \item Nov/17: Finalizing Paper
  \item Dec/1: Presentation
\end{itemize}


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case




\end{document}
