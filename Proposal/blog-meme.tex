% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{Data-Intensive Computing for Text Analysis}{Fall 2011, School of Information, University of Texas at Austin}
\CopyrightYear{2011} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Meme Tracking in Scale with MapReduce}% \titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
\subtitle{Project Proposal}
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Hohyon Ryu\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{hohyon@utexas.edu}
\alignauthor
Jae Hyeon Bae\\
       \affaddr{Computer Science}\\
       \affaddr{University of Texas at Austin}\\
       \email{metacret@gmail.com}
\alignauthor
Nicholas Woodward\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{woodward.nicholas@gmail.com}
}

% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{14 September 2011}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}

This article is to present our project proposal for Fall 2011 Data-Intensive Computing for Text Analysis class. Using MapReduce-based text processing for keyword extraction, possible applications are discussed. First, using implicit links, sparse links between blog articles that hampers applying information retrieval techniques can be overcome. Second, with Latent Dirichlet Allocation, we may generate a topic model based on extracted keywords. Finally, with the extracted keywords we can visualize blogs and keywords that may help perceive how the blogosphere appears in the big picture.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4.m}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{H.5.4}{Information Interfaces and Presentation}{Hypertext/Hypermedia}

\terms{MapReduce, Blogosphere}

\keywords{Keyword Extraction, Data-intensive Processing, Implicit links in Blogosphere, Meme tracking}

\section{Introduction}

Web logs, also known as blogs, are a very important and interesting source of diverse information. They provide insight into the culture of the society in which they exist. People search blogs to find out how to cook, what to do, what to see, and to discover what others think. Because blogs are typically written in plain language by individuals enthusiastic for their interest, they may be more easily understood than longer, more-detailed news articles and cover a wider personal range of topics. However, unlike web information retrieval, blog information retrieval is still very limited and faces many challenges. Some well-established Web information retrieval techniques do not perform well on blogs because of sparse links, multimedia contents and the short length of the contents \cite{Agarwal}. In particular, PageRank \cite{Brin1998a}, which made a revolutionary improvement by taking into account the link structure of HTML Web documents, does not perform well for blog posts as they lack strong link structure. Another problem for blog information retrieval is the length of the blog posts. They vary from one line to several pages and require smoothing or normalization to achieve effective information retrieval. 
	In this study, a novel method for augmenting implicit citation links to the explicit HTML links and document expansion for blog posts will be introduced. Extraction of implicit citation links from blog posts will address the sparse link problem that hampers utilizing PageRank and other link-based retrieval algorithms for blog information retrieval. The methodology of extracting implicit citation will utilize Blog content extraction using Decruft\footnote{http://www.minvolai.com/blog/decruft-arc90s-readability-in-python/} and noun chunk extraction based on Python NLTK\footnote{Natural Language Toolkit, http://www.nltk.org/}. By augmenting each document's implicit links, the proposed method tries to improve blog information retrieval. We will also explore other applications that could potentially improve information presentation for the Blogosphere.


\section{Meme Extraction and Analysis}

\subsection{Test Collection}

BLOGS08\footnote{http://ir.dcs.gla.ac.uk/test\_collections/blogs08info.html} is a TREC\footnote{http://trec.nist.gov/} test collection that crawled Web Logs from Jan/14/2008 to Feb/10/2009. It consists of 3 components: feeds, permalink documents, and homepage documents. We use only permalink documents that contain 28,488,766 documents and whose uncompressed size is 1445GB. Because the collection contains many duplicates, non-English blog posts, advertisement, and Blog website frameworks, we will use NLTK and Decruft to extract English blog contents without boilerplate text.

\subsection{Preprocessing Blogs08}

Preprocessing Blogs08 Collection is done using Python with NLTK and Decruft packages. Decruft extracts only meaningful content from a blog page filtering out navigation links, advertisements, sidebar contents, and link to other contents and sites. Using a language identification module, NLTK checks if the extracted content is written in English. Blogs08 also contains a tremendous amount of duplicate documents and documents without significant amount of text. We set a lower-bound of 5 words and remove duplicates, also using Python. After all these preprocess are completed, every extracted content of a blog post is written into a single line per document for the MapReduce meme extraction process.

\subsection{Meme Extraction}

Preprocessed Blogs08 data is comprised of key-value pair where the key is document number(docno) and the value is the preprocessed HTML content of the document. Using Hadoop Streaming, Python MapReduce code extracts candidate phrases and group documents by the common phrases. The mapper first processes input HTML content using BeautifulSoap\footnote{http://www.crummy.com/software/BeautifulSoup/} and strips all HTML tags and JavaScript code. Using NLTK, the mapper tags all the words' part of speech using HUNPOS-tagger\footnote{http://code.google.com/p/hunpos/} and extracts noun phrases, verb phrases, prepositional phrases, and combinations of all these types. The mapper outputs the text key-value pairs where key is a phrase, and the value is docno. The reducer collects all the docnos that share the same phrase and output the phrase and docnos pair. If the number of documents is less than 5, the reducer ignores the phrase.

\subsection{Meme Clustering and Topic Analysis}
To track mutation and propagation of similar memes, similar memes must be grouped together. For the grouping, we use MapReduce-based clustering. Clustering is the most basic technique to retrieve meaningful information from the corpus. In this project, we will use K-means clustering as the starting point. Because we do not have any prior information of meme distribution, we will apply Canopy clustering \cite{McCallum2000} to choose initial seeds of clustering instead of random seeds selection of K-means clustering. There are two threshold parameters $T_1,T_2,T_1>T_2$ at Canopy clustering. Because this algorithm is pretty fast, we can experiment with various combinations of threshold parameters to get reasonable number of clusters. After K-means clustering with centroids generated by Canopy algorithm, we can start analyzing meme distribution in each cluster and find which memes are releated to each other.
Cosine distance will be used as the distance measure between two memes. Suppose that document $P,Q$ are represented as two vectors $(P_0, P_1,...,P_n)$ and $(Q_0, Q_1,...,Q_n)$, cosine distance between two documents can be expressed mathematically as the following:
\begin{displaymath}
D(P,Q)=1-\frac{\sum(P_i \times Q_i)}{\sqrt{\sum{(P_i)^2} \times \sum{(Q_i)^2}}}
\end{displaymath}
For Canopy generation and K-means clustering, Apache Mahout\footnote{http://mahout.apache.org} implementation will be used. 
Latent Dirichlet Allocation (LDA)\cite{Blei2003} is a generative probabilistic model that can be utilized to detect topics in a document collection. LDA models the document as the random mixture over latent topics, and topic can be represented by a distribution over words. Using LDA, we can create meme mixtures for each topic and topic assignments for memes in each document. We can then group memes with similar topics and analyze several aspects of the data, including topic distrubition over documents and relationshp among topic space. Yahoo! LDA\footnote{https://github.com/shravanmn/Yahoo\_LDA}  implementation will be used. LDA algorithm requires several parameters, of which the most critical is the number of topics that should be specified. This number can be the same as K set with the aforementioned K-means clustering. In order to find the optimal the number of topics we will calculate the perplexity of test collection with the various number of topics. Formally, for a test set of M documents, the perplexity is
\begin{displaymath}
 perplexity(D_{test})=exp\{-\frac{\sum_{d=1}^M log\ p(\mathbf{w}_d)}{\sum_{d=1}^M N_d}\}
\end{displaymath}


\section{Application for Extracted Memes}

\subsection{Meme Tracking}

\subsection{Meme Source Analysis}

\subsection{Improved User Interface}

\subsection{Information Retrieval Performance Improvement}

The idea of using link structure for information retrieval has been recognized as the key to successful retrieval algorithms since the emergence of Google and its core algorithm, PageRank \cite{Brin1998a}. Another effective approach to utilizing links in information retrieval called HITS was proposed around the same time \cite{Kleinberg1999}.  PageRank and HITS are now the minimum standard of the web information retrieval to assure the quality of the retrieved web pages. Unfortunately, these powerful algorithms cannot be fully exploited for blog information retrieval due to lack of strong link structure in the blogosphere. 

With extracted keyphrases using MapReduce and NLTK, we will provide more prior information for a MapReduce-based search engine Ivory\footnote{http://www.umiacs.umd.edu/~jimmylin/ivory/docs/index.html}, to improve blog search performance.


\subsection{Visualization for Users}

There have been many attempts to visualize the blogosphere \cite{Tauro2008, Uchida2007}. However, they were insufficient in helping users with better navigation. Using the visualization techniques integrated into the navigation system, we would be able to benefit actual users by improving the process of finding blog articles, thus allowthing them to more easily navigate between related articles.


\section{Project Plan}

\subsection{Division of Work}

Hohyon Ryu: Meme Extraction, Meme Tracking
Jae Hyeon Bae: Clustering Memes, Scalability Issues
Nicholas Woodward: Application, Theoretical Background, Meme Tracking

\subsection{Timeline}


\begin{itemize}
  \item Oct/6: Preprocessing
  \item Oct/13: Meme Extraction
  \item Oct/20 \& 27: Meme Clustering
  \item Nov/3 \& 10: Meme Tracking and Application
  \item Nov/10: Presentation Webpage Buildup
  \item Nov/17: Finalizing Paper
  \item Dec/1: Presentation
\end{itemize}


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case




\end{document}
