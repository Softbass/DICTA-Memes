% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{Data-Intensive Computing for Text Analysis}{Fall 2011, School of Information, University of Texas at Austin}
\CopyrightYear{2011} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Large-Scale Meme Tracking with MapReduce}% \titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{Progress Report}
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Hohyon Ryu\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{hohyon@utexas.edu}
\alignauthor
Jae Hyeon Bae\\
       \affaddr{Computer Science}\\
       \affaddr{University of Texas at Austin}\\
       \email{metacret@gmail.com}
\alignauthor
Nicholas Woodward\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{woodward.nicholas@gmail.com}
}

% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{09 December 2011}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}

With the recent development of Web 2.0 technologies and the concomitant explosion of user-generated content has come an incredible wealth of textual data whose form and content demand new strategies for text retrieval and classification. In this paper we will describe our approach for extracting phrases that potentially allude to ideas, behaviors or styles that spread from person to person within a culture, collectively known as memes, from a large corpus of blog data. In addition to phrase extraction, we use the canopy clustering approach \cite{McCallum2000} with a modified coefficient to group similar phrases. Two Amazon Mechanical Turk jobs are used to measure the likelihood that the clusters constitute a meme, and these data are used to build feature sets for supervising two support vector machines to classify the remaining clusters.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4.m}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{H.5.4}{Information Interfaces and Presentation}{Hypertext/Hypermedia}

\terms{MapReduce, Blogosphere, Canopy Clustering, Amazon Mechanical Turk, Support Vector Machine, Regression}

\keywords{Keyword Extraction, Data-intensive Processing, Implicit links in Blogosphere, Meme tracking}

\section{Introduction}

Web logs, also known as blogs, are a very important and interesting source of diverse information. They provide insight into the culture of the society in which they exist. People search blogs to find information on a their favorite interests or hobbies, to see what to do or where to go, and to discover what others are talking about. Because blogs are typically written in plain language by individuals enthusiastic about particular topics, they may be more easily understood than longer, more-detailed news articles and cover a wider range of topics. Blogs are also more likely to contain memes, or ideas, behaviors and styles that are popular within a certain culture. Memes can generally be identified by relatively short, distinct phrases that travel from person to person in a society. The fact that blogs are updated regularly makes them a good source for retrieving and extracting memes. In this study we experiment with a method for extracting, clustering and classifying memes from a large corpus of blog data using the Hadoop MapReduce framework \cite{Dean2008}. Our approach builds on previous attempts at mining data from the blogosphere and more traditional text clustering and classification algorithms. Additionally, we incorporate crowdsourced meme ranking data obtained from Amazon's Mechanical Turk (MTurk) \footnote{http://www.mturk.com} service to train two meme classifiers. We conclude with results from our meme rankings and some potential solutions to the challenges posed by research in the blogosphere.

\section{Related Work}
\subsection{Text Retrieval in the Blogosphere}
The universe of blogs, known traditionally as the blogosphere, is an emerging field of research in information retrieval that presents several unique challenges. The blog format is typically very free form, with strong variation in the type, length and format of text \cite{Schmidt2007}. Blogs may contain structured or unstructured information and numerous anomalies in spelling and punctuation that may be intentional or not. The blogosphere is constantly being updated, and thus it is highly sensitive to fast-changing temporal topics. By extracting and tracking memes in the form of phrases that travel across blogs we can mine a variety of issues and fads in the blogosphere.

Compared to web information retrieval, extracting information from blogs is still relatively limited. Several well-established information retrieval techniques do not perform well on the blogosphere due to sparse links, multimedia contents and the variable length of the contents \cite{Agarwal}. In particular, ranking algorithms such as PageRank \cite{Brin1998a} and HyperText Induced Topic Selection (HITS) \cite{Kleinberg1999} are less effective because blogs are rarely linked together and the overall link structure is sparse. Several new blog ranking techniques use characteristics of the blog content, a score to measure the blog's influence, or the originality of the text in the blogosphere to compliment standard link algorithms \cite{Hassan2009,Song2007}. 

Kolak and Schilit \cite{Kolak2008} describe a method to generate links by mining quotations in books. Their approach detects several windows of words that share similar terms and then shingles them together to create longer quotations with a minimum of eight words. Our approach builds off of theirs, but because we are searching for short, distinctive phrases in a noisy environment we opt for n-grams of only three words.
 
\subsection{Crowdsourcing with MTurk}
Our project relies on user input from several MTurk jobs in order to rank a training set of memes and create a feature set for later classification. Amazon's crowdsourcing platform was launched in November, 2005, and there is limited research about its role in scientific experiments. Though the company releases little information regarding the demographics of people who complete tasks on the site, Ross et al. \cite{Ross2010} surveyed workers and found that in the platform's initial phase the overwhelming majority of workers (83\%) were born the United States. But since that time the percentage has fallen, and the authors projected that by 2010 a majority of workers would come from outside the U.S., with India as the largest contributing country.

\subsection{Text Clustering and Classification}
Clustering is a basic data mining tool for discovering topics in a document collection \cite{Jayabharathy2011}. In information retrieval, clustering has a wide range of application including search result clustering, scatter-gather, collection clustering, and cluster-based retrieval \cite{citeulike:2709781}. Clustering can aggregate dispersed information and help clarify how to find access points for information in a large-scale corpus.

In the blogosphere, clustering algorithms have been used to mine information for a variety of purposes. They have been used to detect popular topics and extract trends based on keyword clusters that ebb and flow over time \cite{Qamra2006,Bansal}. Lin, et al. \cite{Lin2007} demonstrate how blogs can be grouped into communities based on common topics. Our approach varies from the others in that we are only concerned with grouping lexically similar memes with a high level of granularity. In our case, we have a large corpus of blog text, the data is very sparse with many features, and we expect a large number of clusters. For these reasons, we use the canopy clustering technique developed by McCallum, Nigem \& Ungar \cite{McCallum2000}.

\subsection{Learning to Rank}
Machine-learned ranking has become a popular topic in information retrieval in the last decade, and there are several algorithms that are potentially applicable to our study \cite{Liu2011}. The algorithms can be categorized into three approaches: pointwise, pairwise and listwise. RankNet \cite{Burges2005} is based on pairwise preference and uses a neural net to learn ranking functions. McRank \cite{Li2007} converts the ranking problem into the multiple classification, where the classification probability is changed to the ranking score using \emph{Expected Relevance}. This method has been shown to outperform two other pairwise preference methods, LambdaRank \cite{Burges2007} and regression-based ranker \cite{springerlink:10.1007/11776420_44}. Drawing on ideas from these approaches we attempted several basic "learning to rank" ideas to classify informative memes in our study.

\section{Meme Extraction}

\subsection{Test Collection}

BLOGS08\footnote{http://ir.dcs.gla.ac.uk/test\_collections/blogs08info.html} is a TREC\footnote{http://trec.nist.gov/} test collection that crawled Web Logs from Jan/14/2008 to Feb/10/2009. It consists of 3 components: feeds, permalink documents, and homepage documents. We use only permalink documents that contain 28,488,766 documents and whose uncompressed size is 1445GB. 

\subsection{Preprocessing Blogs08}

Because the collection contains many duplicates, non-English blog posts, advertisements, and HTML tags, the following preprocessing is applied to Blogs08 collection using Beautiful Soup HTML Parser, NLTK, and Decruft packages.

\subsubsection{Applying Decruft and Removing non-English Posts}
Decruft\footnote{http://www.minvolai.com/blog/decruft-arc90s-readability-in-python/} extracts only meaningful content from a blog page, filtering out navigation links, advertisements, sidebar contents, and links to other sites. Decruft is a Python implementation of ARC90's readability project, and it improves Python-readability in terms of processing speed. 

We used the NLTK\footnote{http://www.nltk.org/} language identification module to determine if the extracted content is written in English. After applying Decruft and removing non-English posts, extracted contents of a blog post are written into a single line per document for the MapReduce meme extraction process. The number of remaining posts after these first two steps was 166,749,81(96 GB).

\subsubsection{Deduplication and HTML Strip}

The Blogs08 data contains many duplicate documents and documents without a significant amount of text. We set a lower-bound of five words and removed duplicates, also using Python. Finding duplicate documents is an $O(n^2)$ operation, which requires a tremendous amount of computation and a long running time. In order to improve efficiency, we applied the following optimizations: first character indexing, similar length indexing, and creating a binary vector of first four characters of the terms.

To avoid having to compare each document with all of the others in the corpus, binary vectors of the documents are stored in files named by the length of the vector and the lowercased first character of the document. Table \ref{table:dedup} shows an example of a blog post that is indexed in file 00030a. "00030" stands for the document length in terms of the number of unique words, and "a" stands for the lowercased first character of the document from "At". Ignoring HTML tags, the first 4 characters of all the words in the document are stored in the index file as a document vector. By using only the first four characters, the vectors use less memory and disk space, and computation for comparison becomes more efficient.

\begin{table}[h!t!]
\begin{center}
\begin{tabular}{l|p{6.0cm}}

\hline
DOC ID & BLOG08-20081015-016-0056752183\\ 

\hline
Document & 

At Work


\emph{(3 pictures omitted)}



(Photos: Zeitgeist Stage technicians take sand from the back alley of the BCA and spread it around the set for the current production of Seascape.)

Labels: At Work, Seascape, Zeitgeist Stage


- posted by Art @ 1:52 PM\\
\hline
File name& 00030a\\
\hline
Vector & and, set, curr, seas, back, at, art, alle, from, arou, for, 1, take, spre, prod, pm,labe, it, stag, post, by, phot, zeit, of, work, bca, 52, sand, tech, the\\

\hline
\end{tabular}
\caption{An example of the index for deduplication. The set of words are stored in a file named by concatenating the length of the word set and the lowercased first letter of the document.}
\label{table:dedup}
\end{center}
\end{table}

Once the vector generation is completed for all of the documents, the deduplication program scans through the vector files, finds duplicate vectors and writes a removal list, which is a list of document ids that should be deleted in the next step. If a vector is shorter than 5, the document id is added to the removal list. The program notes vector files of $\pm 2$ length of the current vector starting with the same character. Document simalarity is measured by the following formula:
\begin{displaymath}
dsim=\frac{A \cap B}  {min(|A|, |B|)}
\end{displaymath}

The denominator $min(|A|, |B|)$ is chosen to remain relatively close to the numerator. Any document having a similarity score, $dsim > 0.85$, with the current document is added to the removal list. In the third step, the entire collection is copied, excluding the documents on the removal list. 1,570,428 documents (9.4\%) were removed in the deduplication process, leaving 15,104,553(85GB) documents for the remaining preprocessing steps.

For the purposes of extracting memes, HTML tags were removed with the Beautiful Soup HTML parser\footnote{http://www.crummy.com/software/BeautifulSoup/} and all text was converted to lowercase. The test collection for meme extraction was reduced to 43GB. HTML tags were preserved separately for later use, because this document collection will be used to present the blog post in a web interface.

\subsection{Meme Extraction with MapReduce}

Preprocessed Blogs08 data is comprised of key-value pairs where the key is document number(doc\_id) and the value is the preprocessed content of the blog posts. Using Hadoop Streaming, Python MapReduce code extracts common phrases and groups documents by them. The overall algorithm design follows the algorithm presented in \cite{Kolak2008}. However, due to a scalability issue, lower bounds and upper bounds limit the output size of the data and the algorithms are simplified.

Our approach utilizes other optimizations such as stop word ratio check and preposition trimming. The three steps of MapReduce pseudocode for meme extraction are illustrated below. The first MapReduce process splits the input text into sentences and extracts trigrams and their indices within the document. It checks if the trigram is comprised with only words. If all of the three words are sto pwords according to a short stop word list shown in Table \ref{table:stopwords}, the trigram is ignored. The reducer removes the phrases($trigrams$) that appear less than 5 times or more than 300,000 times.

\begin{table}[ht]
\begin{center}
\begin{tabular}{p{0.8cm}|p{7.2cm}}

\hline

\hline
Short Stop Word List & a, the, of, in, on, january, february, march, april, may, june, july, august, september, october, november, december, jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec, am, pm \\
\hline
Long Stop Word List & a, about, above, after, again, against, all, also, am, an, another, and, any, are, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can't, cannot, could, couldn't, did, didn't, do, does, doesn't, doing, don't, down, during, each, few, for, from, further, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, he's, her, here, here's, hers, herself, him, himself, his, how, how's, i, i'd, i'll, i'm, i've, if, in, into, is, isn't, it, it's, its, itself, let's, me, more, most, mustn't, my, myself, no, nor, not, of, off, on, once, only, or, other, ought, our, ours , ourselves, already, out, over, own, same, shan't, she, she'd, she'll, she's, should, shouldn't, so, some, such, than, that, that's, the, their, theirs, them, themselves, then, there, there's, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, very, was, wasn't, we, we'd, we'll, we're, we've, were, weren't, what, what's, when, when's, where, where's, which, while, who, who's, whom, why, why's, with, won't, would, wouldn't, you, you'd, you'll, you're, you've, your, yours, yourself, yourselves, january, february, march, april, may, june, july, august, september, october, november, december, jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec\\

\hline
\end{tabular}
\caption{Stop word list for blogs.}
\label{table:stopwords}
\end{center}
\end{table}

\begin{centering}

\begin{tabbing}
\textbf{MapReduce 1:}\\
\emph{Mapper 1:}\\

for \= ($doc\_id$, $text$) in [input file]:\\
\>	$sentences$  $\leftarrow$ split text by sentence boundary\\
\\
\>	for \= $sentence$ in $sentences$:\\
\>\>	$index$ $\leftarrow$ 0\\
\>\>	for \= $trigrams$ in $sentence$:\\
\>\>\>		$index$++	\\
\>\>\>		if \= checkStopwords($trigrams$)<1:	\\
\>\>\>\>		emit ($trigrams$, ($doc\_id$, $index$))\\
\\
\emph{Reducer 1:}\\
$lower\_bound$  $\leftarrow$ 5\\
$upper\_bound$  $\leftarrow$ 300000\\
for \= ($trigrams$, [($doc\_id$, $index$)]) in [mapper 1 output]:\\
\>	$len \leftarrow$ count([($doc\_id$, $index$)])\\
\> 	if \= $lower\_bound$ < $len$ < $upper\_bound$:\\
\>\>	emit ($trigrams$, [($doc\_id$, $index$)])\\
\end{tabbing}
\end{centering}

The second MapReduce process sorts the trigrams and indices by document to concatenate adjacent trigrams in MapReduce 3:

\begin{centering}
\begin{tabbing}
\textbf{MapReduce 2:}\\
\emph{Mapper 2:}\\
for \= ($trigrams$, [($doc\_id$, $index$)]) in [reducer 1 output]:\\
\>	for \= ($doc\_id$, $index$) in [($doc\_id$, $index$)]:\\
\>\> emit ($doc\_id$, ($trigrams$, $index$))\\
\\
\emph{Reducer 2:}\\
for \= ($doc\_id$, [($trigrams$, $index$)]) in [mapper 2 output]:\\
\>	emit ($doc\_id$, [($trigrams$, $index$)])

\end{tabbing}

\end{centering}

The third MapReduce process concatenates the adjacent trigrams within a blog post. Mapper 3 concatenates adjacent trigrams into a meme. Before emitting the memes a function trimPrepositions() removes prepositions at the beginning and end of the meme, and a function stopwordRate() checks how many of the words in the meme are stop words using the long stop word list shown in Table \ref{table:stopwords}. If stop words constitute more than 65\% of the words in the meme, the meme is ignored. In Reducer 3, phrases that are too short ($\leqq 5$) or too long ($\geqq 200$) are ignored as they are often timestamps, advertisements, or boilerplate text. The final MapReduce is illustrated as follows:

\begin{centering}
\begin{tabbing}
\textbf{MapReduce 3:}\\
\emph{Mapper 2:}\\

for \= ($doc\_id$, [($trigrams$, $index$)]) in [reducer 2 output]:\\
\>	$prev\_index=0$\\
\>	$meme=``"$\\
\>	for \= ($trigrams$, $index$) in [($trigrams$, $index$)]:\\
\>\>	if \= $index-prev\_index \leqq 3$:\\
\>\>\>		$meme$ += $trigrams$[3-($index-prev\_index$):]\\
\>\>	else:\\
\>\>\>		$meme$=trimPrepositions($meme$)\\
\>\>\>		if \= stopwordRate($meme$)<0.65:\\
\>\>\>\>		emit($meme$, $doc\_id$)\\
\>\>\>		$meme$=trimPrepositions($meme$)\\
\>\>\>		$meme$.append($meme$)\\
\>\>\>		$meme$=$trigram$\\
\>\>	$prev_index=index$\\
\\	
\>	\# clean up for the last one\\
\>		$meme$ = trimPrepositions ($memes$)\\
\>		if \= stopwordRate($meme$)<0.65:\\
\>\>		emit($meme$, $doc\_id$)\\
\\
\emph{Reducer 3:}\\
for \= ($meme$, [($doc\_id$)]) in [mapper 3 output]:\\
\> if \= $5 \leqq len(meme) \leqq 200$:\\
\>\>	emit ($meme$, [($doc\_id$)])

\end{tabbing}

\end{centering}

The final output are (key, value) pairs of memes and the list of the ids of documents in which the meme appears. In total, we extracted 4,610,100 memes from the TREC Blogs08 collection.

Table \ref{table:memeext} illustrates the parameters used for meme extraction.

\begin{table}[h!t!]
\begin{center}
\begin{tabular}{l|l}

\hline
\textbf{Parameter} & \textbf{Value}\\

\hline
Lowerbound for ngram appearance & 5\\
\hline
Upperbound for ngram appearance & 300,000\\
\hline
Lowerbound for meme length & 5\\
\hline
Upperbound for meme length & 200\\

\hline
Upperbound for stop word rate within a meme & 0.65\\

\hline
\end{tabular}
\caption{The parameters and optimizations used for deduplication.}
\label{table:memeext}
\end{center}
\end{table}

\section{Meme Clustering}

\subsection{Mahout LDA}
Latent Dirichlet Allocation (LDA) characterize the document as the mixture of latent topics and the topic as the distribution of words \cite{Blei2003a}. LDA has shown significant improvement over previous attempts at topic modeling, most notably probabilistic latent semantic analysis \cite{Hofmann1999}. It interprets each document as an unordered bag of words. Each word in the document is generated by first choosing the topic from topic distribution and then choosing the word from the word distribution, given the topic. When we denote the word, topic, document as $w, z, D$, the topic model formulates the following word distribution in the document:
\begin{displaymath}
 P(w)=\sum_{i=1}^K P(w|z_i)P(z_i)
\end{displaymath}
where $K$ is the number of topic for the corpus, $P(z_i)$ is the probability that $i$th topic was sampled for $w$, and $P(w|z_i)$ is the probability of $w$ given the $i$th topic. Using this interpretation, if we find the topic and word distribution for each document to maximize the probability of the entire corpus, we can group memes sharing similar semantical meaning.

Apache Mahout\footnote{http://mahout.apache.org} is an open-source scalable machine learning library that runs on the Hadoop architecture. It includes various machine learning algorithms for tasks such as clustering, classification and recommender systems. Apache Mahout includes an LDA implementation, but it has several drawbacks that led us to opt for a different solution in our study.

\subsubsection{Defect 1: Parameter estimation}
The primary issue with Mahout's LDA implementation is that it does not estimate parameters with variational Expectation Maximization (EM). In variational inference algorithm, the Dirichlet parameters $\alpha, \beta$ should be estimated to maximize likelihood of $p(\mathbf{w}|\alpha,\beta)$. Since a likelihood also is not tractable for computation, variational EM method should be used to maximize the lower bound of a likelihood value. But the Mahout LDA implementation does not estimate parameters and uses a fixed value submitted by the user on the command line as a $\alpha$ and log probability of a word given a topic as $\beta_{iw_n}$. Regarding the effects of fixed vs. optimized parameters, the author of the implementation contends that the decision would not have much of an affect on the basic functions of LDA \footnote{Personal communication.}. We disagree, citing the original LDA algorithm which states that Dirichlet parameters should be learned dynamically \cite{Blei2003a}.

\subsubsection{Defect 2: Convergence}
The original LDA algorithm proposed the EM method to iterate until the lower bound of log likelihood value converged. The expectation step is an inference, and the maximization step is maximizing the lower bound of the log likelihood regarding the model parameters $\alpha, \beta$ using variational parameters obtained from the first step. The Mahout LDA implementation does not execute the maximization step and only repeats the expectation step using the previously calculated log probability of a word if given a topic.

\subsection{Yahoo! LDA}
Due to limitations in the Mahout LDA implementation, we decided to use the Yahoo! LDA implementation\footnote{https://github.com/shravanmn/Yahoo\_LDA}. The Yahoo! LDA implementation is not based on Hadoop MapReduce, in part because of the limited programming paradigm of MapReduce and because its disk I/O between every job is not suitable for iterative machine learning representation. The Yahoo! Implementation of distributed LDA utilizes a message passing approach, as well as Hadoop Streaming and HDFS (Hadoop Distributed File System) for job communication and intermediate shared storage. We attempted to run the Yahoo! LDA with our meme data collection on the Longhorn cluster at TACC, but all efforts were unsuccessful. Currently, the Yahoo! LDA implementation is operating with bash shell script for distributing work and synchronization, and we arrived at the conclusion that it is not fully equipped for a distributed environment. We encountered the following problems during our experiments: permission denied for excutables in jar package, a synchronization problem from global/lda.dict.dump, and an exception, Ice::ConnectionLostException.

We were able to resolve the first two issues by modifying several lines of code in the script files, but we could not resolve the last problem. According to the Yahoo! LDA implementation author\footnote{Personal communication.}, the lost connection problem is caused by Hadoop. The Yahoo! LDA implementation supports checkpointing and restart mechanisms, but when we attempted a restart the environment reverted back to what it was after the first iteration of LDA. The primary difficulty was the large scale of our collection (4 million memes), and many iterations were required to produce meaningful topic collection. On average each iteration ran for seven minutes with a single machine and 1.5 minutes with five machines. The default number of iterations of Yahoo! LDA implementation is 1,000.

\subsection{Mahout Canopy Clustering}
One of the most common and efficient clustering algorithm is $k$-means clustering. The main drawbacks of $k$-means clustering is that the number of clusters $k$ must be specified at runtime, instead of determined dynamically. As such, $k$-means clustering is very sensitive to the initial seed clusters. Unlike $k$-means, canopy clustering generates clusters based on a distance measure. Primarily for this reason we chose to estimate a suitable number of clusters and initial seed clusters using the canopy clustering approach \cite{McCallum2000}. We used the canopy implementation from the Apache Mahout libraries with the Jaccardi distance measure.

Canopy clustering receives 2 distance parameters $t_1, t_2$ such that $t_1 > t_2$. It starts with empty canopy list. If the distance between the vector and the canopy in the canopy list is less than $t_1$, the vector is added to the canopy. The distance from the canopy is the distance from the centroid of vectors in the canopy. If there is no canopy whose distance is less than $t_2$, meaning the point is not strongly bounded for canopies, the new canopy is created with the point. This is repeated for every point.

Our processed blog data contains more than 4.6 million memes, and they are highly sparse. Using default values for the parameters and very generous distance measures for $t_1,t_2$, we anticipated more than one million canopies. Our attempts to run the code caused out of memory errors while running on Hadoop MapReduce. Because the time complexity is near $O(N^2)$ with many canopies, we could not use the original canopy clustering implementation. After numerous unsuccessful attempts we decided to develop a new approach, Indexed MapReduce Canopy.

\subsection{Indexed MapReduce Canopy}
Because the form of memes evolves over time across different sources \cite{Leskovec2009}, we must group similar phrases into meme clusters. For example, "lipstick look good on a pig," "lipstick on the pig," and "lipstick on this pig" need to be seen in the same cluster. A canopy clustering algorithm \cite{McCallum2000} is used for this meme clustering. However, since we are clustering more than 4.6 million memes into many clusters of very fine granularity, different strategies should be adopted. In our case we used canopy clustering as a single pass approach, and the distance between every meme and every cluster is calculated in $O(n^2)$ computational complexity. The meme clustering task is very different from ordinary document clustering as the number of documents is extremely high while the length of each document is usually fewer than a dozen words. We devised the following optimizations and modifications to efficiently cluster 4.6 million memes:
\begin{itemize}
 \item Binary vector: A vector is simply a set of words in a meme or a centroid. The set operation is O(1) while the list operation is O(n). Word order is ignored in this method.
 \item Centroid indexing: The centroid is indexed by the words it includes so that each meme is compared only to the cluster centroids that include common words.
 \item New similarity coefficient: RBS similarity coefficient
 Jaccard's similarity coefficient 
\begin{displaymath} 
 J(A,B) = {{|A \cap B|}\over{|A \cup B|}}
\end{displaymath} 

The Jaccard coefficient is not suitable for our meme clustering as it is highly sensitive to the length difference between memes. We compare memes with cluster centroids, and when a cluster centroid is long, short memes cannot have a high coefficient score and be assigned into the cluster. Therefore, we used a new approach, known as RBW, for the coefficient. Instead of dividing by the union, we divide the number of common words by the average length of the two vectors to reduce sensitivity to length:\\

\begin{displaymath}
RBW(A, B) = {{|A \cap B|} \over {(|A|+|B|) / 2} }
\end{displaymath}

Table \ref{table:sim} shows that Jaccardi's coefficient is much more sensitive to the length difference between a meme and a centroid. By dampening the sensitivity to the length difference, the RBW coefficient is more sensitive to the intersection of their terms. 

\begin{table}[h!t!]
\begin{center}
\begin{tabular}{p{3.0cm}|p{2.2cm}|r|r}
\hline
\textbf{Meme}&\textbf{Centroid}&\textbf{Jaccardi}&\textbf{RBW}\\
\hline
weapons of mass destruction programs&&1.00&1.00\\
\hline
iraqs weapons of mass destruction & weapons mass destruction programs&0.60&0.75\\
\hline
fear of weapons of mass destruction & weapons mass destruction programs iraq&0.43&0.60\\
\hline
weapons of mass destruction and ballistic missiles&weapons mass destruction programs iraq fear&0.37&0.50\\
\hline
\end{tabular}
\caption{Comparison between Jaccardi's coefficient and the proposed RBW coefficient. Jaccardi's coefficient drops quickly as the centroid gets longer. In contrast, the RBW coefficient is less sensitive to the length difference and more sensitive to the intersection of words.}
\label{table:sim}
\end{center}
\end{table}

The similarity score must be higher than 0.6 to be clustered into a given centroid.
 
 \item Stop word removal: Because we intend to compare clusters using only words that could potentially be meaningful, we decided to remove common stop words and other boilerplate text. We want to avoid a scenario in which the phrases "You can put lipstick on a pig" and "You can put a book on the table" would be clustered together. Using an extended stop word list that includes month and weekday names, we exclude stop words from the similarity computation. 
 \item Distributed computation: To cluster 4.6 million memes efficiently we used a MapReduce-based distributed Canopy approach, as explained in Apache Mahout Canopy Clustering\footnote{https://cwiki.apache.org/MAHOUT/canopy-clustering.html}. The mapper takes in a partial list of the memes. Using the canopy algorithm, it generates cluster centroids. The reducer does the same clustering process with the aggregated centroids. The pseudocode of the MapReduce process is illustrated as follows:
 

\begin{centering}
\begin{tabbing}
\textbf{Meme-clustering:}\\
\emph{Mapper:}\\

$CIndex \leftarrow new HashMap$ \# Centroid Index\\
$C \leftarrow new List$ \# Cluster Centroids\\
for \= $meme$ in $memes$:\\
\>	\# Initialize $C$ and $CIndex$\\
\>	\# 0 is the index of the first cluster\\
\>	if \= $C$ is empty:\\
\>\>	\# set() makes a set of words in a meme\\
\>\>	$C$.append( set(meme) )\\
\>\>	for \= $word$ in $meme$:\\
\>\>\>		$CIndex$[$word$].add(0) \\
\\
\>	\# Get candidate centroids from $CIndex$\\
\>	$Candidates \leftarrow new Set()$\\
\>	for \= $word$ in $meme$:\\
\>\>		for \= $c$ in $CIndex$[$word$]:\\
\>\>\>		$Candidates$.add($c$)\\
\\
\>	$flag \leftarrow False$\\
\>	for \= $c$ in $Candidates$:\\
\>\>	if memeSim($meme$, $C$[$c$])>0.6:\\
\>\>\>		$C$[$c$]=$C$[$c$] + set($meme$)\\
\>\>\>	$flag \leftarrow True$\\
\>\>\>		for \= $word$ in $meme$:\\
\>\>\>\>		$CIndex$[$word$].add($c$) \\
\>\>\>		break
\\
\> if \= $flag == False$:\\
\>\>	$C$.append(set($meme$))\\
\>\>	$c$=len($C$)-1\\
\>\>		for \= $word$ in $meme$:\\
\>\>\>		$CIndex$[$word$].add($c$) \\
emit($C$)\\

\\
\emph{Reducer:}\\
\>The reducer is identical to the mapper except that \\
\> the input is the centroids($C$) instead of\\
\> the memes($memes$).\\

\end{tabbing}

\end{centering}

A total of 2,010,355 cluster centroids were generated, and the 4.6 million extracted memes were assigned into these clusters.
 
\end{itemize}


\section{Meme Ranking}
Now that we have successfully clustered the memes, the task remains to classify them according to a meaningfulness or informativeness ranking. Though classification is a challenge, many of the meme clusters can easily be classified as meaningless because they contain sequences of words such as “2 1956 jamboree tues jan 3 1956 jamboree wed,oct 5th wed oct 4 tues” and do not form a coherent idea. If we extract some statistical features from meme clusters and build the training data based on informativeness, we can predict an informativeness score for each meme cluster. Finally, if we build the model of classifier as a regression problem of probability of meaningfulness given a meme cluster, we can bring order into a set of meme clusters with the probability values. Currently, we are expecting the following features to be determinant in deciding a meme cluster's informativeness.
\begin{itemize}
 \item the number of memes in the cluster
 \item average meme length
 \item average number of documents in the cluster
 \item average number of sources in the cluster
 \item meme cluster cohesiveness
 \item time span information of blog documents containing memes in the cluster
  \begin{itemize}
    \item the number of days blog documents were written
    \item the number of documents per day
  \end{itemize}
\end{itemize}
Meme cluster cohesiveness can be defined as 
\begin{displaymath}
\frac{the\ number\ of\ unique\ words}{the\ number\ of\ words}
\end{displaymath}
which means how similar memes in the cluster are each other.

\subsection{Building Training Set with Amazon Mechanical Turk}

\subsubsection{Initial Training Set (MT1)}
Ranking the meme clusters based upon their meaningfulness or informativeness as described above is a difficult and largely subjective task. Consequently, using machine learning for this task is highly problematic. To aid the process of automated classification of the phrase clusters, we implemented an Amazon Mechanical Turk (MTurk) to utilize human intelligence in building a training set \cite{Barr2006}. MTurk is an Internet service from Amazon for scalable crowdsourcing of simple one-time tasks. It allows \emph{requesters} submit human intelligence tasks (HITs) to be completed by \emph{workers} in exchange for small payments. Requesters approve the results they consider satisfactory, and workers increase their confidence scores with high approval rates.

In this MTurk HIT workers were shown a cluster of phrases and three 100-word snippets of text from the blog posts in which the phrases appear, along with questions related to the meaningfulness of the phrases. Workers were asked seven questions about each meme cluster, such as "Is this a meaningful chunk of information?", "Does this meme relate to a specific topic?" and "Is this a popular phrase or expression?".

In total, we submitted 1090 meme clusters to MTurk. Workers were required to have a HIT approval rate greater than 85\% and to have completed at least 50 approved HITs. All meme clusters were reviewed by three workers to allow for varying subjective assessments. After removing the highest and lowest total scores to eliminate any individual biases, we calculated the mean score of the cluster and normalized on a 0-1 scale. Each meme cluster was then compared to a range of threshold values for the purposes of the Support Vector Machine, as described in the following section. The labels collected by this initial MTurk HIT will be referred to as Mechanical Turk Collection 1(MT1).

\subsubsection{Pairwise Training Set (MT2)}
Due to our limited success in classifying meme clusters using SVMs, we attempted another approach similar to the Bradley-Terry model \cite{Bradley1952} for pairwise comparison. For our second MTurk HIT, we submitted 1,492 meme clusters to be compared sequentially by workers. After workers were presented with a definition for memes ("an idea, behavior or style that spreads from person to person within a culture") and examples of good and bad memes, workers were asked to select the better meme cluster. Meme cluster \#1 was compared to cluster \#2, cluster \#2 to cluster \#3 and so on. The labels collected by this pairwise comparison MTurk HIT will be referred to as Mechanical Turk Collection 2(MT2). The results were used to train the Bradley-Terry model, as described in the following section.

\subsubsection{Training Set from Meme Tournament Game (Work In Progress)}
Determining the quality of memes is highly subjective task that involves an understanding of the culture in which they pass from person to person. As such, global MTurk workers may not posses the necessary background knowledge to judge the quality of memes. For example, a worker in India (the country with the highest percentage of MTurk workers) may not recognize a celebrity's name or a TV show reference in another country. The meme, "lipstick on a pig", will only be recognized as a meme if the person is familiar with the political context of the U.S. national election in 2008. Therefore, we have developed a meme comparison game\footnote{http://stingray.ischool.utexas.edu/meme\_tournament/} that may potentially help us to collect meme ranking data, such as the ones shown in Figure \ref{fig:game}. However, due to the fact that our game does not contain contextual information about the memes and is open to the general public, results from this game will not be used to create a training set until further development is completed.

\begin{figure*}[htb]
	\begin{center}
		{\includegraphics[width=\textwidth]{game.jpg}}
	\end{center}
	\caption{A Screen Capture of Meme Browser(MB).}
	\label{fig:game}
\end{figure*}



\subsection{Ranking the Memes with Machine Learning Algorithms}
\subsubsection{Ranking with Support Vector Machine}

The MTurk results yielded 1090 meme clusters with ranking data to be used in creating a feature set for training. The first method for meme ranking is classification using Support Vector Machine. Meme clusters in the training set were classified as positive or negative using a score threshold, and then we calculated the probability that a meme cluster in the training set should be classified in the positive set. Sorting the meme clusters in the training set by the probability value allows us to find the most significant meme clusters. To do this we utilized the LIBSVM package\footnote{http://www.csie.ntu.edu.tw/$\sim$cjlin/libsvm/}. Our first task was to find the best threshold value to classify meme clusters in the training set as positive or negative. If a cluster's score is less than the threshold value then its class is negative. Otherwise it is defined as positive. When we naively chose the threshold value as 0.5, our training data contained only 10\% negative data. The trained model with this data predicted only 2 negative examples for the training set. As a result, we attempted several threshold values that could provide the best ratio of positive/negative data and prediction accuracy. Table \ref{table:svm} shows the results for several threshold values. We used the tool included LIBSVM package to find the best parameters and feature scale.

\begin{table*}[t!hb]
\begin{center}
\begin{tabular}{c|p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}}

Threshold&No. of negative instances in training data&No. of negative instances predicted with the trained model&Cross-Validation accuracy(\%)&Training classification accuracy(\%)\\
\hline
0.5&108&2&90.2\%&90.2\% (984/1090)\\
0.6&321&4&70.8\%&70.9\% (773/1090)\\
0.7&427&38&61.3\%&62.1\% (677/1090)\\
0.75&587&577&56.2\%&72.8\% (794/1090)\\
0.8&639&1090&58.6\%&58.7\% (639/1090)\\
\end{tabular}
\caption{Classification experiment using SVM}
\label{table:svm}
\end{center}
\end{table*}

As we can see in Table \ref{table:svm}, the threshold value 0.75 appears to most adequately suit our needs, while the other values were unable to classify meme clusters as positive or negative. Using a threshold value of 0.75, we calculated the probability of positive classification for each meme cluster and then sorted the results. Figure \ref{fig:svm_result} shows the top 20 meme clusters.

\begin{figure*}[htb]
\begin{center}
\fbox{
\begin{tabular}{p{16cm}}

\begin{enumerate}
\item also offering a 500,are you offering senate candidate 2,offering 10,offering 1,offering 20,offering 30,offering 3,offering 4,offering 5,offering 6
\item am not 's design,decree of god,design to do's,god's decree,god's decree that those,god's design,it's design
\item 0 scientists,2004 scientists used,2008 scientists,400 scientists,january 2009 scientists have managed,the scientists used
\item 10 lessons,2008 lessons,2009 lessons,2 lessons,3 lessons,clarinet lessons 28,forgotten the lessons of 9
\item my own room happy memorable 2007,room 101,room 1,room 205,room 222,room 2,room 401,room 6,room 8
\item he's already gotten,he's gotten,it's gotten,she's gotten,that's gotten a lot of headline,what's gotten our shownotes,shownotes are complete,shownotes
\item 18 ct,1 slice,28 ct,32 ct,40 ct,6175 ne 195 ct,64 slice ct
\item it's sold,my soul's,sold at sotheby's,sold it's soul,s to be sold,the soul's,your soul's
\item energize a,energize,energize our,energize the,energize your,this would energize his supporters
\item a sultry,her sultry,sultry,the sultry,you are vixensexy and sultry
\item an exhortation,been my exhortation continually,exhortation,the exhortation
\item been snacking,be snacking,not too bad for snacking,snacking,while snacking
\item fear of rectum or,her rectum,his rectum,my rectum,the rectum,your rectum
\item cb,cb and so forth,cb,the cb
\item a centrist,conservative and centrist,more centrist,the centrist
\item it's required,required by today's,response to today's,that's required,what's required
\item a disgraceful,are disgraceful to ourselves,disgraceful,his disgraceful,offensive and disgraceful,the disgraceful
\item entitles him,entitles the holder,entitles them,entitles you
\item intimidate and terrorize,terrorize the,terrorize them,to terrorize
\end{enumerate}

\end{tabular}
}
\caption{Top 20 memes by SVM classification probability}
\label{fig:svm_result}
\end{center}
\end{figure*}

Looking at the results, it is apparent that our approach is able to recognize several keywords or phrases in the meme clusters. But this does not necessarily imply that those clusters are meaningful or informative in the same way as popular phrases that achieve critical mass. For example, if we search for known memes such as "lipstick on a pig" in the blog data the probability values of meme clusters containing this phrase are all less than 0.5, implying they are not meaningful or relevant. There are several potential causes for our results, including:
\begin{itemize}
 \item Erroneous feature engineering
 \item Reliability of Amazon Mechanical Turk training set
 \item Wrong approach to ranking the memes
\end{itemize}

More testing needs to be done in order to determine how these potential causes may be affecting our results. Using our current approach as a baseline, we intend to modify several components of the process and compare the new results with those detailed in this paper. In the following section, we detail another approach for ranking our meme clusters using regression.

\subsubsection{Ranking with Support Vector Regression}
Regression consists of estimating the target value based on feature values. Instead of estimating the probability of a meme cluster to be informative, we calculate scores using Support Vector Regression (SVR) to get informative meme clusters using the same training set from SVM experiment (MT1). We used $\epsilon$-SVR. The Root Mean Square Error (RMSE) value for our training set was 0.0171578, a reasonable value. Figure \ref{fig:svr_result} shows the top 20 memes sorted by regression values.


\begin{figure*}[htb]
\begin{center}
\fbox{
\begin{tabular}{p{16cm}}
\begin{enumerate}
\item carisoprodol carisoprodol,carisoprodol posted
\item bonnie and greta,greta
\item a chav,chav paderborn
\item despots and peons,despots
\item actions resulting,resulting actions
\item mysql usr,the usr
\item didn't budge,didn't burn
\item the eleusinian,the eleusinian mysteries
\item chilly powder,red chilly powder
\item a super old,super old
\item a major topic,major topic
\item altered nuclear transfer,nuclear transfer
\item i wantd,just wantd
\item degraw,sackett and degraw
\item the black strat,the strat
\item palatine ill,the palatine
\item johns house,taylor johns house
\item instant messaging systems,messaging systems
\item emil blonsky,soldier emil blonsky
\item diner dash 2,diner dash 1.106850
\end{enumerate}


\end{tabular}
}
\caption{Top 20 memes using Support Vector Regression}
\label{fig:svr_result}
\end{center}
\end{figure*}

Unfortunately, this result does not look so informative either.

\subsubsection{Ranking with Pairwise Comparison}
In the final ranking approach, we used a pairwise comparison model, also known as the Bradley-Terry model \cite{Bradley1952}. The model can be formulated by the following logit-linear form:
\begin{displaymath}
 logit[(P(i\ beats\ j)]=\lambda_i - \lambda_j
\end{displaymath}
where $\lambda_i=log\alpha_i$ for all $i$. If we assume that all comparisons are independent of each other, we can estimate $\lambda_i$ value with maximum likelihood estimate using a generalized linear model. In order to train the Bradley-Terry model, we used data from an MTurk of 1,492 meme pair comparisons from MT2. For each pair (A, B), we will have two training instances. If the evaluator decides A is better than B for the first instance, the class label is 1 and the features are difference of feature values from A to B. For another instance, a class label is -1 and features are difference from B to A. Using this evaluation approach with our training data, we learned the weights of different features using logistic regression. In this case, the bias should always be 0 because there are an equal number of instances in each class. Using the weights learned with this model, we can calculate the score of each meme cluster as a weighted sum of feature values. The results of this model, as shown in Figure \ref{fig:bt_result}, are superior to our results from ranking with support vector regression. In order to smooth the differences between feature values, we applied the $log$ function to the following features:

\begin{itemize}
 \item the number of memes in the cluster
 \item average meme length
 \item average number of documents in the cluster
 \item average number of sources in the cluster
 \item the number of days blog documents were written
\end{itemize}

\begin{figure*}[htb]
\begin{center}
\fbox{
\begin{tabular}{p{16cm}}

\begin{enumerate}

\item they instruct the computer
\item ground of disfavored victimized by god
\item or other highly technological systems within their power
\item gods hate these
\item you grow and age it can create birth defects affect cellular development
\item asians into the united
\item expect etruscan strongholds
\item disfavored cities and the southern
\item a process of masculinization
\item the mediterreanean region is grossly disfavored
\item greed generate problems with money glorify overconsumption etc
\item 1492 exodus from spain spain became evil financed columbus initiated missionaries usa
\item centuries ago ruled with an iron fist
\item italian food creates fat people with stomach
\item be those whom traditionally maintained this type
\item gibralter broke through inundating the basin and killing untold
\item rich cultures helped the people understand and pursue lives
\item historical pattern and you will suffer for exploiting the
\item disfavored girls
\item i think these are the true candidates for immortality
\end{enumerate}
\end{tabular}
}
\caption{Top 20 memes using the Bradley-Terry Model}
\label{fig:bt_result}
\end{center}
\end{figure*}

We built a web interface to our meme clusters named Meme Browser(MB)\footnote{http://stingray.ischool.utexas.edu/meme\_browser/} that highlights memes and provides a list of documents containing the meme and a timeline of the cluster. Figure \ref{fig:mockup} shows a sample MB page that presents a blog post related to a meme cluster "lipstick on a pig." At the top of the page, MB provides the histogram of the number of documents that contain memes in the same cluster. In this example it is noteworthy that the phrase "the lipstick on a pick" gained political connotation and became very popular in September 2008, and was subsequently discussed quite often for approximately another month before becoming infrequent again. The red bars below the timeline histogram show other memes in the cluster, the period of time they appear in the data, and the number of times they were mentioned in 2008 and early 2009. The histogram and the related memes support the “onMouseover” Javascript function to provide detailed information as shown in the screen shot. The main column displays an article containing the meme. With the help of the Decruft library and CSS processing, only the contents of the article are shown with a consistent format. The right column shows the list of blogs in which the meme appeared. Using this interface, further evaluation will be done to determine the quality of extracted memes, and it will be used to help annotators judge the meaningfulness of memes in context.
\begin{figure*}[htb]
	\begin{center}
		{\includegraphics[width=\textwidth]{mockup.jpg}}
	\end{center}
	\caption{A Screen Capture of Meme Browser(MB).}
	\label{fig:mockup}
\end{figure*}

\section{Conclusion}
The issues surrounding information retrieval in the blogosphere are not likely to disappear anytime soon. With billions of blog posts each year, approaches aimed at deriving high quality information from blogs will require large-scale solutions that account for the particularities of the medium. In our project, we utilized Hadoop MapReduce, crowdsourced rankings, and machine learning to find informative memes from a large-scale blog collection. While the previous body of literature focused on extracting named entities and abstract topics from blogs, we attempted to extract meaningful chunks of information known as memes and evaluate them using machine learning technology powered by human intelligence gathered from an Amazon Mechanical Turk. In the process, we successfully completed massive text preprocessing to extract memes and devised a novel indexed approach for canopy clustering.

We used the data from the Mechanical Turk to create a feature set for training two types of support vector machines. The quality of this task is crucially dependent on not only a rigorous approach to feature engineering but also on using the appropriate machine learning framework. Our classification and regression approaches did not work as well as we had hoped. However, with better modeling, a larger training set and improved feature engineering , we are confident that the quality of our results will improve. The first step will be evaluating memes in context with our Meme Browser to build a better training set. Additionally, time series analysis will improve our representation of memes in the blogosphere by allowing us to demonstrate how they shift over time.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case




\end{document}
