% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{Data-Intensive Computing for Text Analysis}{Fall 2011, School of Information, University of Texas at Austin}
\CopyrightYear{2011} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Meme Tracking in Scale with MapReduce}% \titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{Progress Report}
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Hohyon Ryu\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{hohyon@utexas.edu}
\alignauthor
Jae Hyeon Bae\\
       \affaddr{Computer Science}\\
       \affaddr{University of Texas at Austin}\\
       \email{metacret@gmail.com}
\alignauthor
Nicholas Woodward\\
       \affaddr{School of Information}\\
       \affaddr{University of Texas at Austin}\\
       \email{woodward.nicholas@gmail.com}
}

% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{14 September 2011}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}

This article is to report our project progress for Fall 2011 Data-Intensive Computing for Text Analysis class. We have completed preprocessing of the Blogs08 collection by removing all HTML tags, boilerplate text, non-English documents, and duplicates. After the preprocessing, aproximately 4 million popular common phrases were extracted using an enhanced version of the technique suggested by \cite{Kolak2008}. The extracted phrases are clustered in 2 million meme groups. The meme groups will be ranked automatically using Support Vector Machine, based on the crowdsourced labels obtained from an Amazon Mechanical Turk.

\end{abstract}

% A category with the (minimum) three required fields
\category{H.4.m}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{H.5.4}{Information Interfaces and Presentation}{Hypertext/Hypermedia}

\terms{MapReduce, Blogosphere}

\keywords{Keyword Extraction, Data-intensive Processing, Implicit links in Blogosphere, Meme tracking}

\section{Introduction}

Web logs, also known as blogs, are a very important and interesting source of diverse information. They provide insight into the culture of the society in which they exist. People search blogs to find out how to cook, what to do, what to see, and to discover what others think. Because blogs are typically written in plain language by individuals enthusiastic for their interest, they may be more easily understood than longer, more-detailed news articles and cover a wider personal range of topics. 
As blogs are highly sensitive to fast-changing temporal issues, finding and tracking memes, pieces of information that travel across blog posts, we can mine variety of issues and fads in the blogosphere. In this project we will extract memes from blogs, cluster them by their similarity, and rank the meme clusters using crowdsourcing-powered machine learning.

\section{Related Work}

\section{Meme Extraction}

\subsection{Test Collection}

BLOGS08\footnote{http://ir.dcs.gla.ac.uk/test\_collections/blogs08info.html} is a TREC\footnote{http://trec.nist.gov/} test collection that crawled Web Logs from Jan/14/2008 to Feb/10/2009. It consists of 3 components: feeds, permalink documents, and homepage documents. We use only permalink documents that contain 28,488,766 documents and whose uncompressed size is 1445GB. Because the collection contains many duplicates, non-English blog posts, advertisement, and Blog website frameworks, we will use NLTK and Decruft to extract English blog contents without boilerplate text.

\subsection{Preprocessing Blogs08}

Preprocessing of the Blogs08 Collection was completed using Python with NLTK and Decruft packages. Decruft extracts only meaningful content from a blog page, filtering out navigation links, advertisements, sidebar contents, and links to other sites. NLTK uses a language identification module to determine if the extracted content is written in English. Blogs08 also contains a tremendous amount of duplicate documents and documents without a significant amount of text. We set a lower-bound of 5 words and removed duplicates, also using Python. After all these preprocessing steps were completed, extracted contents of a blog post are written into a single line per document for the MapReduce meme extraction process. The number of Decruft-applied, deduplicated, and HTML stripped English blog posts is 15,097,266 (43 GB).

\begin{itemize}
\item English only
\item Decruft
\item Deduplication
\item HTML Strip
\end{itemize}


\subsection{Meme Extraction with MapReduce}

Preprocessed Blogs08 data is comprised of key-value pair where the key is document number(doc\_id) and the value is the preprocessed content of the blog posts. Using Hadoop Streaming, Python MapReduce code extracts common phrases and group documents by them. Overall algorithm design follows that of \cite{Kolak2008}. However, because of a scalability issue, lower bounds and upper bounds limit the output size of the data, and other optimizations are used such as stop word ratio check and preposition trimming. The three steps of MapReduce pseudo-code for Meme Extraction are illustrated in the following pseudo codes. The first MapReduce process splits the input text into sentences and extracts trigrams and their indexes within the document. The reducer removes the common phrases($trigrams$) that appear less than 5 times or more than 300,000 times.

\begin{centering}

\begin{tabbing}
\textbf{MapReduce 1:}\\
\emph{Mapper 1:}\\

for \= ($doc\_id$, $text$) in [input file]:\\
\>	$sentences$  $\leftarrow$ split text by sentence boundary\\
\\
\>	for \= $sentence$ in $sentences$:\\
\>\>	$index$ $\leftarrow$ 0\\
\>\>	for \= $trigrams$ in $sentence$:\\
\>\>\>		$index$++	\\
\>\>\>		emit ($trigrams$, ($doc\_id$, $index$))\\
\\
\emph{Reducer 1:}\\
$lower\_bound$  $\leftarrow$ 5\\
$upper\_bound$  $\leftarrow$ 300000\\
for \= ($trigrams$, [($doc\_id$, $index$)]) in [mapper 1 output]:\\
\>	$len \leftarrow$ count([($doc\_id$, $index$)])\\
\> 	if \= $lower\_bound$ < $len$ < $upper\_bound$:\\
\>\>	emit ($trigrams$, [($doc\_id$, $index$)])\\
\end{tabbing}
\end{centering}

The second MapReduce process sorts the trigrams and indices by document to concatenate adjacent trigrams in MapReduce 3:

\begin{centering}
\begin{tabbing}
\textbf{MapReduce 2:}\\
\emph{Mapper 2:}\\
for \= ($trigrams$, [($doc\_id$, $index$)]) in [reducer 1 output]:\\
\>	for \= ($doc\_id$, $index$) in [($doc\_id$, $index$)]:\\
\>\> emit ($doc\_id$, ($trigrams$, $index$))\\
\\
\emph{Reducer 2:}\\
for \= ($doc\_id$, [($trigrams$, $index$)]) in [mapper 2 output]:\\
\>	emit ($doc\_id$, [($trigrams$, $index$)])

\end{tabbing}

\end{centering}

The third MapReduce process concatenates the adjacent trigrams within a blog post. Mapper 3 concatenates adjacent trigrams into a meme. Before emitting the memes, a function trimPrepositions trims off prepositions at the beginning and end of the meme, and a function stopwordRate checks how many of the words in the meme are stop words. If stop words constitute more than 65\% of the words in the meme, the meme is ignored. In the reducer 3, phrases that are too short ($\leqq 5$) or too long ($\geqq 200$) are ignored as they are often timestamps, advertisement, or boiler plate text. The final MapReduce is illustrated as follows:

\begin{centering}
\begin{tabbing}
\textbf{MapReduce 3:}\\
\emph{Mapper 2:}\\

for \= ($doc\_id$, [($trigrams$, $index$)]) in [reducer 2 output]:\\
\>	$prev\_index=0$\\
\>	$meme=``"$\\
\>	for \= ($trigrams$, $index$) in [($trigrams$, $index$)]:\\
\>\>	if \= $index-prev\_index \leqq 3$:\\
\>\>\>		$meme$ += $trigrams$[3-($index-prev\_index$):]\\
\>\>	else:\\
\>\>\>		$meme$=trimPrepositions($meme$)\\
\>\>\>		if \= stopwordRate($meme$)<0.65:\\
\>\>\>\>		emit($meme$, $doc\_id$)\\
\>\>\>		$meme$=trimPrepositions($meme$)\\
\>\>\>		$meme$.append($meme$)\\
\>\>\>		$meme$=$trigram$\\
\>\>	$prev_index=index$\\
\\	
\>	\# clean up for the last one\\
\>		$meme$ = trimPrepositions ($memes$)\\
\>		if \= stopwordRate($meme$)<0.65:\\
\>\>		emit($meme$, $doc\_id$)\\
\\
\emph{Reducer 3:}\\
for \= ($meme$, [($doc\_id$)]) in [mapper 3 output]:\\
\> if \= $5 \leqq len(meme) \leqq 200$:\\
\>\>	emit ($meme$, [($doc\_id$)])

\end{tabbing}

\end{centering}

The final output is key, value pairs of a meme and the list of the document ids that the meme appears. In total, 4,610,100 memes are extracted from the TREC Blogs08 collection.

\section{Meme Clustering}

\subsection{Mahout LDA}
Latent Dirichlet Allocation (LDA) \cite{Blei2003} characterize the document as the mixture of latent topics and the topic as the distribution of words. This is the most advanced topic model and interprets the document as an unordered bag of words. Each word in the document is generated by first choosing the topic from topic distribution and choosing the word from the word distribution given the topic. When we denote the word, topic, document as $w, z, D$, the topic model formulates the following word distribution in the document:
\begin{displaymath}
 P(w)=\sum_{i=1}^K P(w|z_i)P(z_i)
\end{displaymath}
where $K$ is the number of topic for the corpus, $P(z_i)$ is the probability that $i$th topic was sampled for $w$, and $P(w|z_i)$ is the probability of $w$ given the $i$th topic. Using this interpretation, if we find the topic and word distribution for each document to maximize the probability of entire corpus, we can group memes sharing similar sematical meaning.

Apache Mahout\footnote{http://mahout.apache.org} is the open source scalable machine learning library running based on Hadoop. It has various machine learning algorithm covering from clustering, recommendation, and classification. Apache Mahout has a LDA implementation but it has several drawbacks we decided to choose for our experiment.

\subsubsection{Defect 1: Parameter estimation}
The first defect is it does not estimate parameters with variational Expectation Maximization (EM). In variational inference algorithm, Dirichlet parameters $\alpha, \beta$ should be estimated to maximize likelihood of $p(\mathbf{w}|\alpha,\beta)$. Since a likelihood also is not tractable for computation, variational EM method should be used, which is to maximize the lower bound of a likelihood value. But this implementation does not esitamate parameters and uses a fixed value submitted by user as a $\alpha$ and log probability of a word given a topic as $\beta_{iw_n}$. We asked to the author of this implementation about effects of parameter estimation, he answered that it would not affect much for basic working of LDA. If we increase a topic smoothing parameter, it increases the effect of words that occur infrequently.

\subsubsection{Defect 2: Convergence}
The original algorithm proposed EM method to iterate until the lower bound of log likelihood value converges. E step is inference, and M step is maximizing lower bound of the log likelihood regarding the model parameters $\alpha, \beta$ using variational parameters obtained from E step. Mahout implementation does not execute M step and only repeats E step using previous calculated log probability of a word given a topic.

\subsection{Yahoo! LDA}
Due to above major drabacks of Mahout LDA implementation, we decided to use Yahoo! LDA implementation\footnote{https://github.com/shravanmn/Yahoo\_LDA}. Yahoo! LDA implementation is not based on Hadoop MapReduce because MapReduce is a quite restrictive programming model and its disk IO between every job is not suitable for iterative machine learning representation. Yahoo! implemented distributed LDA computation using message passing way and it uses Hadoop Streamining and HDFS (Hadoop Distributed File System) as job communication and intermediate shared storage. We tried Yahoo! LDA for our meme collection but we failed to successfully run with the distributed environment on Longhorn cluster. Currently, Yahoo! LDA is operating with bash shell script for distributing work and synchronization and we reached to the conclusion that it is not fully packeged for distributed environment. The followings are problems we found during the experiment.
\begin{itemize}
 \item Permission denited for excutables in jar package
 \item Synchronization problem of global/lda.dict.dump
 \item Ice::ConnectionLostException problem 
\end{itemize}
We could resolve first two problems by fixing some lines of script files but we could not resolve the last problem. According to the late response of Yahoo! LDA author, lost connection problem is caused by Hadoop. Yahoo! LDA supports checkpointing and restart mechanism but when we restarted it, it started from the first iteration of LDA. The main difficulty was the large scale of meme collection and many iterations were needed for gaining meaningful topic collection. One iteration took average 7 minutes with a single machine and took average 1.5 minutes with 5 machines. Default number of iterations of Yahoo! LDA implementation is 1,000.

\subsection{Mahout Canopy Clustering}
If we can represent our data as a vector, the most common and efficient clustering algorithm is $K$-means clustering. The main drawbacks of $k$-means clustering is that we have to specify the number of cluster $k$, a priori information of data distribution and $k$-means clustering is very sensitive to initial seed clusters. Thus, there are many suggestions that we can estimate suitable number of clusters and initial seed clusters using canopy clustering \cite{McCallum2000} because canopy clustering is generating clusters totally based on distance measure. Because Apache Mahout contains canopy clustering implementation, we implemented Jacadi distance measure to Apache Mahout interface and executed Mahout canopy clustering with some parameters. Canopy clustering receives 2 distance parameters $t_1, t_2$ such that $t_1 > t_2$. It starts with empty canopy list. If the distance between the vector and the canopy in the canopy list is less than $t_1$, the vector is added to the canopy. The distance from the canopy is the distance from the centroid of vectors in the canopy. If there is no canopy whose distance is less than $t_2$, which means the point is not strongly bounded for canopies, the new canopy is created with the point. This is repeated for every point.
We have more than 4 million memes and they are highly sparse. Without any tuning of canopy clustering algorithm, even though we specify very generous parameters for $t_1,t_2$, we would have more than million canopies it caused out of memory error while running on Hadoop MapReduce. Also, since its time complexity is near $O(N^2)$ with many canopies, we could not use the original canopy clustering implementation. After these trials and errors, we decided to develop Indexed MapReduce Canopy.

\subsection{Indexed MapReduce Canopy}
Since the form of memes evolve over time across different media \cite{Leskovec2009}, similar memes need to be clustered together. For example, ``lipstick look good on a pig," ``lipstick on the pig," and "lipstick on this pig" need to be seen in the same cluster. A canopy clustering algorithm \cite{McCallum2000} is used for this meme clustering. However, since we are clustering more than 4.6 million memes into many clusters of very fine granularity, different strategies should be adopted. Since Canopy is basically a single pass approach, the distance between every meme and every cluster needs to be calculated in $O(n^2)$ computational complexity. The meme clustering task is very different from an ordinary document clustering task as the number of documents is extremely high while the length of each document is usually fewer than a dozen of words. We devised the following optimizations and modifications to efficiently cluster 4.6 million memes:
\begin{itemize}
 \item Binary vector: A vector is simply a set of words in a meme or a centroid. The set operation is O(1) while list operation is O(n). Also word order is ignored by this method.
 \item Centroid indexing: The centroid is indexed by the words it includes so that when a meme is compared to only with the cluster centroids that include common words.
 \item New similarity coefficient: Jaccard similarity coefficient 
\begin{displaymath} 
 J(A,B) = {{|A \cap B|}\over{|A \cup B|}}
\end{displaymath} 
  is inappropriate for meme clustering. We compare a meme with a cluster centroid, and when a cluster centroid is long, short memes cannot have high coefficient scores. Therefore, instead of dividing by the union, we divide the number of common words by the average length of the two vectors:\\
\begin{displaymath}
memeSim (A, B) = {{|A \cap B|} \over {(|A|+|B|) / 2} }
\end{displaymath}
The similarity score should be higher than 0.6 to be clustered into a given centroid.
 
 \item Stop word removal: The memes should be compared by meaningful words instead of stop words. Otherwise, ``You can put lipstick on a pig'' and ``You can put a book on the table" would be clustered together. Using an extended stop word list that includes month names and weekday names, we excluded stop words from the similarity computation. 
 \item Distributed computation: To cluster 4.6 millions of memes efficiently, mapreduce-based distributed Canopy approach, explained in Apache Mahout Canopy Clustering\footnote{https://cwiki.apache.org/MAHOUT/canopy-clustering.html}, is used. The mapper takes in a partial list of the memes. Using canopy algorithm, it generates cluster centroids. The reducer does the same clustering process with the aggregated centroids. The pseudo code of the MapReduce process is illustrated as follows:
 

\begin{centering}
\begin{tabbing}
\textbf{Meme-clustering:}\\
\emph{Mapper:}\\

$CIndex \leftarrow new HashMap$ \# Centroid Index\\
$C \leftarrow new List$ \# Cluster Centroids\\
for \= $meme$ in $memes$:\\
\>	\# Initialize $C$ and $CIndex$\\
\>	\# 0 is the index of the first cluster\\
\>	if \= $C$ is empty:\\
\>\>	\# set() makes a set of words in a meme\\
\>\>	$C$.append( set(meme) )\\
\>\>	for \= $word$ in $meme$:\\
\>\>\>		$CIndex$[$word$].add(0) \\
\\
\>	\# Get candidate centroids from $CIndex$\\
\>	$Candidates \leftarrow new Set()$\\
\>	for \= $word$ in $meme$:\\
\>\>		for \= $c$ in $CIndex$[$word$]:\\
\>\>\>		$Candidates$.add($c$)\\
\\
\>	$flag \leftarrow False$\\
\>	for \= $c$ in $Candidates$:\\
\>\>	if memeSim($meme$, $C$[$c$])>0.6:\\
\>\>\>		$C$[$c$]=$C$[$c$] + set($meme$)\\
\>\>\>	$flag \leftarrow True$\\
\>\>\>		for \= $word$ in $meme$:\\
\>\>\>\>		$CIndex$[$word$].add($c$) \\
\>\>\>		break
\\
\> if \= $flag == False$:\\
\>\>	$C$.append(set($meme$))\\
\>\>	$c$=len($C$)-1\\
\>\>		for \= $word$ in $meme$:\\
\>\>\>		$CIndex$[$word$].add($c$) \\
emit($C$)\\

\\
\emph{Reducer:}\\
\>The reducer is indentical to the mapper except that \\
\> the input is the centroids($C$) instead of\\
\> the memes($memes$).\\

\end{tabbing}

\end{centering}

 
\end{itemize}


\section{Meme Ranking}
Now that we have successfully clustered the memes, we still have many things to do in order to find valuable and meaningful information from the meme clusters. Essentially, we can classify every meme cluster as informative or not because there are many meaningless sequence of words such as ''2 1956 jamboree tues jan 3 1956 jamboree wed,oct 5th wed oct 4 tues``. If we extract some statistical features from meme clusters and build the training data indicating informativeness, we can predict informativeness of each meme cluster. Finally, if we build the model of classifier as a regression problem of probability of informativeness given a meme cluster, we can bring order into a set of meme clusters with the probability values. Currently, we are expecting the following features as discriminant to decide a meme cluster's informativeness.
\begin{itemize}
 \item the number of memes in the cluster
 \item average meme length
 \item average number of documents in the cluster
 \item average number of sources in the cluster
 \item meme cluster cohesiveness
 \item time span information of blog documents containing memes in the cluster
  \begin{itemize}
    \item the number of days blog documents were written
    \item the number of documents per day
  \end{itemize}
\end{itemize}
Meme cluster cohesiveness can be defined as 
\begin{displaymath}
\frac{the\ number\ of\ unique\ words}{the\ number\ of\ words}
\end{displaymath}
which means how similar memes in the cluster are each other.

\subsection{Building Training Set using Amazon Mechanical Turk}

Ranking the meme clusters based upon their meaningfulness or informativeness as described above is a difficult and largely subjective task. Consequently, using machine learning for this task is highly problematic. To aid the process of automated classification of the phrase clusters, we implemented an Amazon Mechanical Turk (MTurk) to utilize human intelligence in building a training set \cite{Barr&Cabrera2006}.  MTurk is an Internet service for scalable crowdsourcing of simple one-time tasks. It allows requesters submit a human intelligence tasks (HIT) to be completed by workers in exchange for small payments.  
   In our study, we displayed to the user a cluster of phrases and three 100-word snippets of text from the blog data in which the phrases appear, along with seven questions, or HITs, related to the meaningfulness of the phrases. Questions ranged from "Is this a meaningful chunk of information?" to "Is this a common expression that could potentially appear elsewhere?"  In total, we submitted 1090 meme clusters to MTurk with seven HITs each. Each cluster was reviewed by three workers to allow for varying subjective assessments.  After removing the highest and lowest total scores, each meme cluster was then compared to a range of threshold values for the purposes of the Support Vector Machine, as described in the following section.

\subsection{Automatic Meme Ranking using Support Vector Machine}
In training data, each meme cluster contains several scores by different evaluators. We remove the highest and lowest scores for each meme cluster to remove evaluator's biases. Then we calculate the mean score and normalize to 0-1 scale. This score will be the target value of ranking.

\subsubsection{Classification using Support Vector Machine}
As a result of the Mechanical Turk, we got 1090 training instances. The first method for meme ranking is classification using Support Vector Machine. We classify meme clusters in the training set as positive or negative with the score threshold and then we calculate the probability that meme cluster in the test set is classfied as the positive set. If we sort meme clusters in the test set by the probability value, we can find significant meme clusters. We utilize the LIBSVM package\footnote{http://www.csie.ntu.edu.tw/~cjlin/libsvm/} for the experiment.
First of all, we have to find the threshold value to classify training instance as positive or negative. If the score is less than the treshold value, its class is negative otherwise positive. When we naively choose the threshold value as 0.5, our training data contains only 10\% negative data and the trained model with this data predicts only 2 negative examples for the training data. So, we find the best threshold value which can balance between ratio of positive/negative data and prediction accuracy. Table \ref{table:svm} is showing the result for various threshold values. We use the tool included LIBSVM package to find the best parameters and feature scale.

\begin{table*}[h!b!p!]
\begin{center}
\begin{tabular}{|c|p{3.0cm}|p{3.0cm}|p{3.0cm}|p{3.0cm}|p{3.0cm}|}
\hline
threshold&No. of negative instances in training data&No. of negative instances predicted with the trained model&Cross-Validation accuracy(\%)&training classification accuracy(\%)\\
\hline
0.5&108&2&90.27&90.27(984/1090)\\
0.6&321&4&70.83&70.91(773/1090)\\
0.7&427&38&61.38&62.11(677/1090)\\
0.75&587&577&56.24&72.84(794/1090)\\
0.8&639&1090&58.62&58.72(639/1090)\\
\hline
\end{tabular}
\caption{Classification experiment using SVM}
\label{table:svm}
\end{center}
\end{table*}

As we can see from Table \ref{table:svm}, threshold value 0.75 looks the best and other threshold value cannot learn to classify negative examples or positive examples at all. Armed with 0.75 as our threshold value, we calculated the probability of positive classification for each meme cluster and sort. Table \ref{table:svm_result} is showing the top 20 memes.

\begin{table*}[h!b!p!]
\begin{center}
\begin{tabular}{|p{15cm}|}
\hline
also offering a 500,are you offering senate candidate 2,offering 10,offering 1,offering 20,offering 30,offering 3,offering 4,offering 5,offering 6\\
am not 's design,decree of god,design to do's,god's decree,god's decree that those,god's design,it's design\\
0 scientists,2004 scientists used,2008 scientists,400 scientists,january 2009 scientists have managed,the scientists used\\
10 lessons,2008 lessons,2009 lessons,2 lessons,3 lessons,clarinet lessons 28,forgotten the lessons of 9\\
my own room happy memorable 2007,room 101,room 1,room 205,room 222,room 2,room 401,room 6,room 8\\
he's already gotten,he's gotten,it's gotten,she's gotten,that's gotten a lot of headline,what's gotten our shownotes,shownotes are complete,shownotes\\
18 ct,1 slice,28 ct,32 ct,40 ct,6175 ne 195 ct,64 slice ct\\
it's sold,my soul's,sold at sotheby's,sold it's soul,s to be sold,the soul's,your soul's\\
energize a,energize,energize our,energize the,energize your,this would energize his supporters\\
a sultry,her sultry,sultry,the sultry,you are vixensexy and sultry\\
an exhortation,been my exhortation continually,exhortation,the exhortation\\
been snacking,be snacking,not too bad for snacking,snacking,while snacking\\
fear of rectum or,her rectum,his rectum,my rectum,the rectum,your rectum\\
cb,cb and so forth,cb,the cb\\
a centrist,conservative and centrist,more centrist,the centrist\\
it's required,required by today's,response to today's,that's required,what's required\\
a disgraceful,are disgraceful to ourselves,disgraceful,his disgraceful,offensive and disgraceful,the disgraceful\\
entitles him,entitles the holder,entitles them,entitles you\\
intimidate and terrorize,terrorize the,terrorize them,to terrorize\\
\hline
\end{tabular}
\caption{Top 20 memes by classificatioin probability}
\label{table:svm_result}
\end{center}
\end{table*}

From the result, we can catch several keywords but it looks not so informative. When we search ''lipstick on the pig`` from the result, probability values of meme clusters containinig ''lipstick on the pig`` were all less than 0.5, which means these were all non-informative memes. We can analyze causes as the following:
\begin{itemize}
 \item Errorneous feature engineering
 \item Reliability of Amazon Mechanical Turk training set
 \item Wrong approach of ranking
\end{itemize}

Because evaluating feature engineering and training set is not easy, we try to rank meme clusters using regression methid.

\subsubsection{Ranking with Support Vector Regression}
Regression is estimating the target value based on feature values. Instead of estimatimg probability of meme cluster to be informative, if we calculate scores using Support Vector Regression (SVR), we can get informative meme clusters. We used $\epsilon$-SVR. Root Mean Square Error (RMSE) value for training set is 0.0171578, reasonable value. Table \ref{table:svr_result} is showing top 20 memes sorted by regression values.

\begin{table*}[h!b!p!]
\begin{center}
\begin{tabular}{|p{15cm}|}
\hline
carisoprodol carisoprodol,carisoprodol posted\\
bonnie and greta,greta\\
a chav,chav paderborn\\
despots and peons,despots\\
actions resulting,resulting actions\\
mysql usr,the usr\\
didn't budge,didn't burn\\
the eleusinian,the eleusinian mysteries\\
chilly powder,red chilly powder\\
a super old,super old\\
a major topic,major topic\\
altered nuclear transfer,nuclear transfer\\
i wantd,just wantd\\
degraw,sackett and degraw\\
the black strat,the strat\\
palatine ill,the palatine\\
johns house,taylor johns house\\
instant messaging systems,messaging systems\\
emil blonsky,soldier emil blonsky\\
diner dash 2,diner dash 1.106850\\
\hline
\end{tabular}
\caption{Top 20 memes by SVR}
\label{table:svr_result}
\end{center}
\end{table*}

Unfortunately, this result does not look so informative. We might have to change our feature engineering.

\section{Evaluation}

Based on the labels collected with the Amazon Mechanical Turk, the performance on SVM-based ranking will be evaluated.

\section{Conclusion}


\section{Logistics}

\subsection{Division of Work}

\begin{itemize}
\item Hohyon Ryu: Data Preprocessing, Meme Extraction, Meme Clustering
\item Jae Hyeon Bae: Theoretical Background, SVM-based Meme Ranking
\item Nicholas Woodward: Theoretical Background, Crowdsourcing the meme cluster evaluation, Mock-up Webpage Build
\end{itemize}

\subsection{Timeline}


\begin{itemize}
  \item Oct/6: Preprocessing - Done
  \item Oct/13: Meme Extraction - Done
  \item Oct/20 \& 27: Meme Clustering - Done
  \item Nov/3 \& 10: Crowdsourcing 
  \item Nov/10: SVM-based Meme Cluster Ranking
  \item Nov/17: Finalizing Paper
  \item Dec/1: Presentation
\end{itemize}


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case




\end{document}
